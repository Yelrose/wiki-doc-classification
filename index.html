<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <title>Wikipedia Document Classification by anuragxel</title>
  </head>

  <body>

    <header>
      <div class="container">
        <h1>Wikipedia Document Classification</h1>
        <h2></h2>

        <section id="downloads">
          <a href="https://github.com/anuragxel/wiki-doc-classification/zipball/master" class="btn">Download as .zip</a>
          <a href="https://github.com/anuragxel/wiki-doc-classification/tarball/master" class="btn">Download as .tar.gz</a>
          <a href="https://github.com/anuragxel/wiki-doc-classification" class="btn btn-github"><span class="icon"></span>View on GitHub</a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">
        <h1>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h1>

<p>In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. We tried to use different kinds of models and study their behaviour to understand the differences in various kinds of topic models, their advantages and their disadvantages while also learning about advances in this field in the last couple of years.</p>

<h1>
<a id="dataset" class="anchor" href="#dataset" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Dataset</h1>

<p>We extracted articles from <a href="http://nlp.uned.es/social-tagging/wiki10+/">Wiki10+</a> using xmltree, bleach and couple of handcrafted regexs. Also, we extracted the top tag associated with them so that each document has a tag associated with them, this will be our topic. We reduced the number of topics from 470 to 24 to make it a feasible classification problem.</p>

<h1>
<a id="methods-used" class="anchor" href="#methods-used" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Methods Used</h1>

<h2>
<a id="tf---idf" class="anchor" href="#tf---idf" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>TF - IDF</h2>

<p>A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. There are various schemes for determining the value that each entry in the matrix should take, and we firstly use the tf-idf formulation. </p>

<p>tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The number of times a term occurs in a document is called its term frequency while inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely. tf–idf is the product of two statistics, term frequency and inverse document frequency.</p>

<h2>
<a id="latent-dirichlet-allocation" class="anchor" href="#latent-dirichlet-allocation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Latent Dirichlet Allocation</h2>

<p>Latent Dirichlet allocation is a generative  model that allows sets of observations to be explained by unobserved groups (latent groups) that explain why some parts of the data are similar. If observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics.</p>

<h2>
<a id="word2vec" class="anchor" href="#word2vec" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>word2vec</h2>

<p>Word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep nets can understand.</p>

<p>Word2vec’s applications extend beyond parsing sentences in the wild. It can be applied just as well to genes, code, playlists, social media graphs and other verbal or symbolic series in which patterns may be discerned. </p>

<h2>
<a id="doc2vecparagraph2vec" class="anchor" href="#doc2vecparagraph2vec" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>doc2vec/paragraph2vec</h2>

<p>The main purpose of Doc2Vec is associating arbitrary documents with labels, so labels are required. Doc2vec is an extension of word2vec that learns to correlate labels and words, rather than words with other words. The first step is coming up with a vector that represents the “meaning” of a document, which can then be used as input to a supervised machine learning algorithm to associate documents with labels.</p>

<h1>
<a id="tech-used" class="anchor" href="#tech-used" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Tech used</h1>

<p>nltk, scikit-learn, gensim, bleach</p>

<p>Huge shoutout to the library developers! :)</p>

<h1>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors and Contributors</h1>

<p><a href="https://github.com/anuragxel" class="user-mention">@anuragxel</a>, <a href="https://github.com/NarendraBabu-U" class="user-mention">@NarendraBabu-U</a>, <a href="https://github.com/harvydent" class="user-mention">@harvydent</a> have contributed to this project.</p>

<h1>
<a id="tags" class="anchor" href="#tags" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Tags</h1>

<p>'Information Retrieval and Extraction Course', 'IIIT-H', 'Major Project', 'Wikipedia', 'Topic Modelling', 'Document Classification', 'Deep Learning', 'word2vec', 'doc2vec', 'tf-idf', 'lda', 'nltk', 'python', 'parsing', 'stemming', 'bleach'</p>
      </section>
    </div>

    
  </body>
</html>
