{
  "name": "Wikipedia Document Classification",
  "tagline": "",
  "body": "# Introduction\r\n\r\nIn machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. We tried to use different kinds of models and study their behaviour to understand the differences in various kinds of topic models, their advantages and their disadvantages while also learning about advances in this field in the last couple of years.\r\n\r\n# Dataset\r\n\r\nWe extracted articles from [Wiki10+](http://nlp.uned.es/social-tagging/wiki10+/) using xmltree, bleach and couple of handcrafted regexs. Also, we extracted the top tag associated with them so that each document has a tag associated with them, this will be our topic. We reduced the number of topics from 470 to 24 to make it a feasible classification problem.\r\n\r\n# Methods Used\r\n\r\n## TF - IDF\r\n\r\nA document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. There are various schemes for determining the value that each entry in the matrix should take, and we firstly use the tf-idf formulation. \r\n\r\ntf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The number of times a term occurs in a document is called its term frequency while inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely. tf–idf is the product of two statistics, term frequency and inverse document frequency.\r\n\r\n## Latent Dirichlet Allocation\r\n\r\nLatent Dirichlet allocation is a generative  model that allows sets of observations to be explained by unobserved groups (latent groups) that explain why some parts of the data are similar. If observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics.\r\n\r\n## word2vec\r\n\r\nWord2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep nets can understand.\r\n\r\nWord2vec’s applications extend beyond parsing sentences in the wild. It can be applied just as well to genes, code, playlists, social media graphs and other verbal or symbolic series in which patterns may be discerned. \r\n\r\n## doc2vec/paragraph2vec\r\n\r\nThe main purpose of Doc2Vec is associating arbitrary documents with labels, so labels are required. Doc2vec is an extension of word2vec that learns to correlate labels and words, rather than words with other words. The first step is coming up with a vector that represents the “meaning” of a document, which can then be used as input to a supervised machine learning algorithm to associate documents with labels.\r\n\r\n# Tech used\r\n\r\nnltk, scikit-learn, gensim, bleach\r\n\r\nHuge shoutout to the library developers! :)\r\n\r\n# Authors and Contributors\r\n\r\n@anuragxel, @NarendraBabu-U have contributed to this project.\r\n\r\n# Tags\r\n\r\n'Information Retrieval and Extraction Course', 'IIIT-H', 'Major Project', 'Wikipedia', 'Topic Modelling', 'Document Classification', 'Deep Learning', 'word2vec', 'doc2vec', 'tf-idf', 'lda', 'nltk', 'python', 'parsing', 'stemming', 'bleach'",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}